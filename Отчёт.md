**Что было сделано**
Был создан парсер, который вычленяет цитаты, автора и теги, а также записывает их в json файл
**Откуда были получены данные**
Данные были получены с сайта https://quotes.toscrape.com/
**Как осуществлялся сбор**
Сбор осуществлялся следующим образом
Используемые библиотеки: BeautifulSoup4, requests, json, os:
```
from bs4 import BeautifulSoup
import requests
import json
import os
```
Сначала указывается количество страниц на сайте, чтобы алгоритм проанализировал их всех:
```
for i in range(1,11):
    Url=f'https://quotes.toscrape.com/page/{i}'
```
С помощью GET запроса библиотеки requests получил страницу:
```
Page=requests.get(Url)
```
Был создана строка для хранения HTML кода текущей страницы, а также создан список для хранения цитат, авторов и тегов:
```
FullHTML=""                
Quotes=[]
```
Далее был создан объект класса BeautifulSoup, в который передаётся код HTML страницы и тип парсера:
```
    Soup = BeautifulSoup(Page.text, "html.parser")
```
После этого в переменная FullHTML записывается результат метода Soup.findAll(), который ищет совпадения по тегу div и классу quote, все совпадения записываются.
```
FullHTML = Soup.findAll('div', class_='quote',itemtype="http://schema.org/CreativeWork")
```
Следующим шагом алгоритм находит тег span и класс text, которые не пусты, затем записываем в список объект типа словарь, где есть 3 ключа - Quote, Author, Tags. Поиск их значений осуществляется методом find, с аргументами span,text/small,author/meta,keywords.
```
 for data in FullHTML:                          
        if data.find('span',class_ = 'text') is not None:
        Quotes.append(
        {'Quote':data.find('span',class_='text').get_text().replace("\u201c",'').replace('\u201d',''),
'Author':data.find('small',class_='author').get_text(),
'Tags':data.find('meta',class_='keywords').get('content')
})
```
Теперь, создаём список, в который помещается копия файла, но сначала проверяем файл на то, что он существует, не пуст и не повреждён, иначе создаём пустой список.
```
if os.path.exists('data.json') and os.path.getsize('data.json') > 0:
        with open('data.json','r') as file:
            try:
                d= json.load(file)
            except json.JSONDecodeError:
                d= []
    else:
        d= []
```
Чтобы в JSON файле был один список, в котором хранятся словари создаётся цикл поэлементно записывающий в копию файла новые данные.
```
for element in Quotes:
        d.append(element)
```
Перезаписываем полученный список в файл
```
with open('data.json','w') as file:
        json.dump(d,file,indent= 1)
```
**Почему был выбран тот или иной метод/инструмент, а не другой**
Были выбраны популярные библиотеки Python, которые подходят для данной задачи.
Библиотека BeautifulSoup4 была использована из-за своей удобства, простоты и подходящей функциональности для большинства задач. По сравнению с другой библиотекой html5lib, которая может быть быстрее и точнее в некоторых ситуациях, она требует дополнительных настроек.
По такому же принципу была выбрана библиотека requests, которая упрощает читаемость кода по сравнению с библиотекой urllib, которая требует более детальной настройки.
Json - это стандартная библиотека python, которую рекомендуется использовать для работы с JSON файлами.
Библиотека os была выбрана, так как она кроссплатформенная, а также поддерживается самими разработчиками Python.
Количество страниц было выявлено вручную, вкратце, во избежание ещё большего спагетти кода